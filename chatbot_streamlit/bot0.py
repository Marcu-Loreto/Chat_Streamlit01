import os
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# Carrega vari√°veis do 
load_dotenv()

#Para rodar o sistema: streamlit run bot.py

# Valida a API key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEY n√£o encontrada. Crie um arquivo .env com OPENAI_API_KEY=suachave")
    st.stop()

# Instancia o cliente usando a chave do ambiente (sem expor no c√≥digo)
client = OpenAI(api_key=OPENAI_API_KEY)

st.write("## ChatBot com IA - Suporte")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FUN√á√ïES RAG - RETRIEVAL AUGMENTED GENERATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@st.cache_data
def carregar_base_conhecimento(caminho_arquivo="conhecimento/base_conhecimento.txt"):
    """
    Carrega e processa a base de conhecimento de um arquivo .txt
    """
    try:
        Path(caminho_arquivo).parent.mkdir(parents=True, exist_ok=True)
        
        # Se arquivo n√£o existir, cria com conte√∫do exemplo
        if not os.path.exists(caminho_arquivo):
            conteudo_exemplo = """STREAMLIT - GUIA DE DESENVOLVIMENTO

1. INSTALA√á√ÉO E CONFIGURA√á√ÉO
Para instalar o Streamlit use: pip install streamlit
Para executar uma aplica√ß√£o: streamlit run app.py
A aplica√ß√£o roda por padr√£o na porta 8501

2. COMPONENTES B√ÅSICOS
st.write() - Exibe texto, markdown, dataframes
st.title() - T√≠tulo principal da p√°gina
st.header() - Cabe√ßalhos de se√ß√£o
st.subheader() - Subcabe√ßalhos
st.text() - Texto simples sem formata√ß√£o
st.markdown() - Texto com formata√ß√£o markdown

3. WIDGETS DE ENTRADA
st.text_input() - Campo de texto simples
st.text_area() - √Årea de texto multilinha
st.number_input() - Entrada num√©rica
st.slider() - Controle deslizante
st.selectbox() - Lista suspensa
st.multiselect() - Sele√ß√£o m√∫ltipla
st.checkbox() - Caixa de sele√ß√£o
st.radio() - Bot√µes de op√ß√£o

4. SESSION STATE
st.session_state permite manter dados entre execu√ß√µes
Para inicializar: if "chave" not in st.session_state: st.session_state.chave = valor
Para acessar: st.session_state.chave
Para modificar: st.session_state.chave = novo_valor

5. LAYOUTS E ORGANIZA√á√ÉO
st.sidebar - Barra lateral para controles
st.columns() - Colunas lado a lado
st.container() - Container para agrupar elementos
st.expander() - Se√ß√£o expans√≠vel
st.tabs() - Abas para organizar conte√∫do

6. CACHE E PERFORMANCE
@st.cache_data - Para cachear dados (recomendado)
@st.cache_resource - Para recursos como modelos ML
st.rerun() - Reinicia a execu√ß√£o da aplica√ß√£o

7. CHAT INTERFACE
st.chat_message() - Container para mensagens de chat
st.chat_input() - Input espec√≠fico para chat
Usar com avatars: st.chat_message("user", avatar="üë§")

8. PROBLEMAS COMUNS
Erro de rerrun infinito: evite modificar session_state em loops
Performance lenta: use cache adequadamente
Layout quebrado: organize com containers e colunas

9. DEPLOY
Streamlit Community Cloud - deploy gratuito
Heroku - para aplica√ß√µes mais robustas
Docker - para containeriza√ß√£o
AWS/GCP - para produ√ß√£o escal√°vel

10. BOAS PR√ÅTICAS
- Use session_state para dados persistentes
- Implemente cache para opera√ß√µes custosas
- Organize c√≥digo em fun√ß√µes
- Use sidebar para configura√ß√µes
- Teste responsividade em dispositivos m√≥veis"""
            
            with open(caminho_arquivo, 'w', encoding='utf-8') as f:
                f.write(conteudo_exemplo)
            st.info(f"üìö Base de conhecimento criada em: {caminho_arquivo}")
        
        # L√™ e processa o conte√∫do
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            conteudo = f.read()
        
        # Divide em blocos (par√°grafos)
        blocos = [bloco.strip() for bloco in conteudo.split('\n\n') if bloco.strip()]
        
        # Divide em senten√ßas tamb√©m
        sentencas = []
        for bloco in blocos:
            # Remove numera√ß√£o e formata√ß√£o
            bloco_limpo = re.sub(r'^\d+\.\s*', '', bloco)
            # Divide em senten√ßas
            sents = [s.strip() for s in bloco_limpo.split('\n') if s.strip()]
            sentencas.extend(sents)
        
        return {
            'blocos': blocos,
            'sentencas': sentencas,
            'texto_completo': conteudo
        }
        
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar base de conhecimento: {str(e)}")
        return {'blocos': [], 'sentencas': [], 'texto_completo': ''}

def calcular_similaridade(texto1, texto2):
    """Calcula similaridade entre dois textos usando SequenceMatcher"""
    # Converte para min√∫sculas e remove caracteres especiais
    t1 = re.sub(r'[^\w\s]', '', texto1.lower())
    t2 = re.sub(r'[^\w\s]', '', texto2.lower())
    
    return SequenceMatcher(None, t1, t2).ratio()

def buscar_contexto_relevante(pergunta, base_conhecimento, max_resultados=3):
    """
    Busca os trechos mais relevantes na base de conhecimento
    usando similaridade de texto simples
    """
    if not base_conhecimento['blocos']:
        return "Nenhuma informa√ß√£o dispon√≠vel na base de conhecimento."
    
    resultados = []
    
    # Busca em blocos (par√°grafos)
    for bloco in base_conhecimento['blocos']:
        similaridade = calcular_similaridade(pergunta, bloco)
        if similaridade > 0.1:  # Threshold m√≠nimo
            resultados.append((similaridade, bloco, 'bloco'))
    
    # Busca em senten√ßas individuais
    for sentenca in base_conhecimento['sentencas']:
        if len(sentenca) > 20:  # Ignora senten√ßas muito curtas
            similaridade = calcular_similaridade(pergunta, sentenca)
            if similaridade > 0.15:  # Threshold um pouco maior para senten√ßas
                resultados.append((similaridade, sentenca, 'sentenca'))
    
    # Ordena por similaridade (maior primeiro)
    resultados.sort(key=lambda x: x[0], reverse=True)
    
    # Pega os melhores resultados
    contexto_relevante = []
    for i, (sim, texto, tipo) in enumerate(resultados[:max_resultados]):
        contexto_relevante.append(f"[Relev√¢ncia: {sim:.2f}] {texto}")
    
    if contexto_relevante:
        return "\n\n".join(contexto_relevante)
    else:
        return "N√£o foi encontrada informa√ß√£o relevante na base de conhecimento para esta pergunta."
   #++++++++++++++++++++++++++++++++++++++++++++++++++++++
   # Prompt
   #   ++++++++++++++++++++++++++++++++++++++++++++++++
   @st.cache_data
def carregar_prompt_do_arquivo(caminho_arquivo="prompts/prompt_sistema.txt"):
    """Carrega o prompt do sistema de um arquivo externo"""
    try:
        Path(caminho_arquivo).parent.mkdir(parents=True, exist_ok=True)
        
        if not os.path.exists(caminho_arquivo):
            prompt_padrao = """Voc√™ √© um assistente de suporte t√©cnico especializado em Streamlit e desenvolvimento Python.

Instru√ß√µes importantes:
- SEMPRE use as informa√ß√µes fornecidas no CONTEXTO para responder
- Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o tem essa informa√ß√£o na base de conhecimento
- Seja preciso e t√©cnico, mas mantenha linguagem clara
- Forne√ßa exemplos de c√≥digo quando apropriado
- Cite as fontes do contexto quando poss√≠vel

Formato de resposta:
1. Responda a pergunta usando o CONTEXTO fornecido
2. Se houver c√≥digo relevante, inclua exemplos
3. Se n√£o houver informa√ß√£o suficiente no contexto, seja honesto sobre isso"""
            
            with open(caminho_arquivo, 'w', encoding='utf-8') as f:
                f.write(prompt_padrao)
        
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            prompt = f.read().strip()
            
        return prompt
        
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar prompt: {str(e)}")
        return "Voc√™ √© um assistente t√©cnico. Use sempre o contexto fornecido para responder."

@st.cache_data
def carregar_configuracao_json(caminho_arquivo="config/bot_config.json"):
    """Carrega configura√ß√µes do bot de um arquivo JSON"""
    try:
        Path(caminho_arquivo).parent.mkdir(parents=True, exist_ok=True)
        
        if not os.path.exists(caminho_arquivo):
            config_padrao = {
                "temperatura_padrao": 0.1,  # Mais baixa para RAG
                "max_tokens_padrao": 500,   # Maior para respostas com contexto
                "modelo_padrao": "gpt-4o",
                "max_contexto_rag": 3,
                "threshold_similaridade": 0.15
            }
            
            with open(caminho_arquivo, 'w', encoding='utf-8') as f:
                json.dump(config_padrao, f, indent=2, ensure_ascii=False)
        
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        return config
        
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar configura√ß√£o: {str(e)}")
        return {"temperatura_padrao": 0.1, "max_tokens_padrao": 500, "modelo_padrao": "gpt-4o"}


# Mem√≥ria de conversa
if "lista_mensagens" not in st.session_state:
    st.session_state["lista_mensagens"] = []

# Exibe hist√≥rico
for msg in st.session_state["lista_mensagens"]:
    st.chat_message(msg["role"]).write(msg["content"])

# Entrada do usu√°rio
mensagem_usuario = st.chat_input("Escreva sua mensagem aqui")

if mensagem_usuario:
    # Guarda e mostra a mensagem do usu√°rio
    st.session_state["lista_mensagens"].append({"role": "user", "content": mensagem_usuario})
    st.chat_message("user").write(mensagem_usuario)

    # Chamada ao modelo
    resposta = client.chat.completions.create(
        model="gpt-4o",
        messages=st.session_state["lista_mensagens"],
        #prompt = " Voce √© um assitente de suporte de uma empresa de software. Responda de forma clara e objetiva.",
        #temperature=0.2,
        #max_tokens=300,
    )

    resposta_ia = resposta.choices[0].message.content

    # Guarda e mostra a resposta da IA
    st.chat_message("assistant").write(resposta_ia)
    st.session_state["lista_mensagens"].append({"role": "assistant", "content": resposta_ia})
