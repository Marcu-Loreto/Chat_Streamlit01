import os
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
import json
from pathlib import Path
import re
from difflib import SequenceMatcher

# Carrega variÃ¡veis do .env
load_dotenv()

# Para rodar o sistema: streamlit run bot.py

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNÃ‡Ã•ES RAG - RETRIEVAL AUGMENTED GENERATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@st.cache_data
def carregar_base_conhecimento(caminho_arquivo="conhecimento/base_conhecimento.txt"):
    """
    Carrega e processa a base de conhecimento de um arquivo .txt
    """
    try:
        Path(caminho_arquivo).parent.mkdir(parents=True, exist_ok=True)
        
        # Se arquivo nÃ£o existir, cria com conteÃºdo exemplo
        if not os.path.exists(caminho_arquivo):
            conteudo_exemplo = """STREAMLIT - GUIA DE DESENVOLVIMENTO

1. INSTALAÃ‡ÃƒO E CONFIGURAÃ‡ÃƒO
Para instalar o Streamlit use: pip install streamlit
Para executar uma aplicaÃ§Ã£o: streamlit run app.py
A aplicaÃ§Ã£o roda por padrÃ£o na porta 8501

2. COMPONENTES BÃSICOS
st.write() - Exibe texto, markdown, dataframes
st.title() - TÃ­tulo principal da pÃ¡gina
st.header() - CabeÃ§alhos de seÃ§Ã£o
st.subheader() - SubcabeÃ§alhos
st.text() - Texto simples sem formataÃ§Ã£o
st.markdown() - Texto com formataÃ§Ã£o markdown

3. WIDGETS DE ENTRADA
st.text_input() - Campo de texto simples
st.text_area() - Ãrea de texto multilinha
st.number_input() - Entrada numÃ©rica
st.slider() - Controle deslizante
st.selectbox() - Lista suspensa
st.multiselect() - SeleÃ§Ã£o mÃºltipla
st.checkbox() - Caixa de seleÃ§Ã£o
st.radio() - BotÃµes de opÃ§Ã£o

4. SESSION STATE
st.session_state permite manter dados entre execuÃ§Ãµes
Para inicializar: if "chave" not in st.session_state: st.session_state.chave = valor
Para acessar: st.session_state.chave
Para modificar: st.session_state.chave = novo_valor

5. LAYOUTS E ORGANIZAÃ‡ÃƒO
st.sidebar - Barra lateral para controles
st.columns() - Colunas lado a lado
st.container() - Container para agrupar elementos
st.expander() - SeÃ§Ã£o expansÃ­vel
st.tabs() - Abas para organizar conteÃºdo

6. CACHE E PERFORMANCE
@st.cache_data - Para cachear dados (recomendado)
@st.cache_resource - Para recursos como modelos ML
st.rerun() - Reinicia a execuÃ§Ã£o da aplicaÃ§Ã£o

7. CHAT INTERFACE
st.chat_message() - Container para mensagens de chat
st.chat_input() - Input especÃ­fico para chat
Usar com avatars: st.chat_message("user", avatar="ğŸ‘¤")

8. PROBLEMAS COMUNS
Erro de rerrun infinito: evite modificar session_state em loops
Performance lenta: use cache adequadamente
Layout quebrado: organize com containers e colunas

9. DEPLOY
Streamlit Community Cloud - deploy gratuito
Heroku - para aplicaÃ§Ãµes mais robustas
Docker - para containerizaÃ§Ã£o
AWS/GCP - para produÃ§Ã£o escalÃ¡vel

10. BOAS PRÃTICAS
- Use session_state para dados persistentes
- Implemente cache para operaÃ§Ãµes custosas
- Organize cÃ³digo em funÃ§Ãµes
- Use sidebar para configuraÃ§Ãµes
- Teste responsividade em dispositivos mÃ³veis"""
            
            with open(caminho_arquivo, 'w', encoding='utf-8') as f:
                f.write(conteudo_exemplo)
            st.info(f"ğŸ“š Base de conhecimento criada em: {caminho_arquivo}")
        
        # LÃª e processa o conteÃºdo
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            conteudo = f.read()
        
        # Divide em blocos (parÃ¡grafos)
        blocos = [bloco.strip() for bloco in conteudo.split('\n\n') if bloco.strip()]
        
        # Divide em sentenÃ§as tambÃ©m
        sentencas = []
        for bloco in blocos:
            # Remove numeraÃ§Ã£o e formataÃ§Ã£o
            bloco_limpo = re.sub(r'^\d+\.\s*', '', bloco)
            # Divide em sentenÃ§as
            sents = [s.strip() for s in bloco_limpo.split('\n') if s.strip()]
            sentencas.extend(sents)
        
        return {
            'blocos': blocos,
            'sentencas': sentencas,
            'texto_completo': conteudo
        }
        
    except Exception as e:
        st.error(f"âŒ Erro ao carregar base de conhecimento: {str(e)}")
        return {'blocos': [], 'sentencas': [], 'texto_completo': ''}

def calcular_similaridade(texto1, texto2):
    """Calcula similaridade entre dois textos usando SequenceMatcher"""
    # Converte para minÃºsculas e remove caracteres especiais
    t1 = re.sub(r'[^\w\s]', '', texto1.lower())
    t2 = re.sub(r'[^\w\s]', '', texto2.lower())
    
    return SequenceMatcher(None, t1, t2).ratio()

def buscar_contexto_relevante(pergunta, base_conhecimento, max_resultados=3):
    """
    Busca os trechos mais relevantes na base de conhecimento
    usando similaridade de texto simples
    """
    if not base_conhecimento['blocos']:
        return "Nenhuma informaÃ§Ã£o disponÃ­vel na base de conhecimento."
    
    resultados = []
    
    # Busca em blocos (parÃ¡grafos)
    for bloco in base_conhecimento['blocos']:
        similaridade = calcular_similaridade(pergunta, bloco)
        if similaridade > 0.1:  # Threshold mÃ­nimo
            resultados.append((similaridade, bloco, 'bloco'))
    
    # Busca em sentenÃ§as individuais
    for sentenca in base_conhecimento['sentencas']:
        if len(sentenca) > 20:  # Ignora sentenÃ§as muito curtas
            similaridade = calcular_similaridade(pergunta, sentenca)
            if similaridade > 0.15:  # Threshold um pouco maior para sentenÃ§as
                resultados.append((similaridade, sentenca, 'sentenca'))
    
    # Ordena por similaridade (maior primeiro)
    resultados.sort(key=lambda x: x[0], reverse=True)
    
    # Pega os melhores resultados
    contexto_relevante = []
    for i, (sim, texto, tipo) in enumerate(resultados[:max_resultados]):
        contexto_relevante.append(f"[RelevÃ¢ncia: {sim:.2f}] {texto}")
    
    if contexto_relevante:
        return "\n\n".join(contexto_relevante)
    else:
        return "NÃ£o foi encontrada informaÃ§Ã£o relevante na base de conhecimento para esta pergunta."

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNÃ‡Ã•ES DE CONFIGURAÃ‡ÃƒO (mantidas do cÃ³digo anterior)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@st.cache_data
def carregar_prompt_do_arquivo(caminho_arquivo="prompts/prompt_sistema.txt"):
    """Carrega o prompt do sistema de um arquivo externo"""
    try:
        Path(caminho_arquivo).parent.mkdir(parents=True, exist_ok=True)
        
        if not os.path.exists(caminho_arquivo):
            prompt_padrao = """VocÃª Ã© um assistente de suporte tÃ©cnico especializado em Streamlit e desenvolvimento Python.

InstruÃ§Ãµes importantes:
- SEMPRE use as informaÃ§Ãµes fornecidas no CONTEXTO para responder
- Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga que nÃ£o tem essa informaÃ§Ã£o na base de conhecimento
- Seja preciso e tÃ©cnico, mas mantenha linguagem clara
- ForneÃ§a exemplos de cÃ³digo quando apropriado
- Cite as fontes do contexto quando possÃ­vel

Formato de resposta:
1. Responda a pergunta usando o CONTEXTO fornecido
2. Se houver cÃ³digo relevante, inclua exemplos
3. Se nÃ£o houver informaÃ§Ã£o suficiente no contexto, seja honesto sobre isso"""
            
            with open(caminho_arquivo, 'w', encoding='utf-8') as f:
                f.write(prompt_padrao)
        
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            prompt = f.read().strip()
            
        return prompt
        
    except Exception as e:
        st.error(f"âŒ Erro ao carregar prompt: {str(e)}")
        return "VocÃª Ã© um assistente tÃ©cnico. Use sempre o contexto fornecido para responder."

@st.cache_data
def carregar_configuracao_json(caminho_arquivo="config/bot_config.json"):
    """Carrega configuraÃ§Ãµes do bot de um arquivo JSON"""
    try:
        Path(caminho_arquivo).parent.mkdir(parents=True, exist_ok=True)
        
        if not os.path.exists(caminho_arquivo):
            config_padrao = {
                "temperatura_padrao": 0.1,  # Mais baixa para RAG
                "max_tokens_padrao": 500,   # Maior para respostas com contexto
                "modelo_padrao": "gpt-4o",
                "max_contexto_rag": 3,
                "threshold_similaridade": 0.15
            }
            
            with open(caminho_arquivo, 'w', encoding='utf-8') as f:
                json.dump(config_padrao, f, indent=2, ensure_ascii=False)
        
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        return config
        
    except Exception as e:
        st.error(f"âŒ Erro ao carregar configuraÃ§Ã£o: {str(e)}")
        return {"temperatura_padrao": 0.1, "max_tokens_padrao": 500, "modelo_padrao": "gpt-4o"}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURAÃ‡ÃƒO INICIAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Valida a API key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("OPENAI_API_KEY nÃ£o encontrada. Crie um arquivo .env com OPENAI_API_KEY=suachave")
    st.stop()

# Instancia o cliente
client = OpenAI(api_key=OPENAI_API_KEY)

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="ChatBot RAG - Suporte",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Carrega dados
config = carregar_configuracao_json()
prompt_sistema = carregar_prompt_do_arquivo()
base_conhecimento = carregar_base_conhecimento()

# TÃ­tulo principal
st.write("## ğŸ¤– ChatBot com RAG - Suporte TÃ©cnico")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SIDEBAR - CONFIGURAÃ‡Ã•ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.sidebar.write("### âš™ï¸ ConfiguraÃ§Ãµes RAG")

# Status da base de conhecimento
total_blocos = len(base_conhecimento['blocos'])
total_sentencas = len(base_conhecimento['sentencas'])

st.sidebar.metric("ğŸ“š Blocos de conhecimento", total_blocos)
st.sidebar.metric("ğŸ“ SentenÃ§as disponÃ­veis", total_sentencas)

# ConfiguraÃ§Ãµes do RAG
max_contexto = st.sidebar.slider(
    "ğŸ” MÃ¡x. resultados RAG:",
    1, 5, 
    config.get("max_contexto_rag", 3),
    help="Quantos trechos relevantes incluir no contexto"
)

# ParÃ¢metros do modelo
st.sidebar.write("### ğŸ›ï¸ ParÃ¢metros do Modelo")

temperatura = st.sidebar.slider(
    "ğŸŒ¡ï¸ Temperatura:",
    0.0, 1.0, 
    config.get("temperatura_padrao", 0.1), 
    0.1,
    help="Mais baixa = mais focada no contexto"
)

max_tokens = st.sidebar.number_input(
    "ğŸ“Š MÃ¡ximo de Tokens:",
    100, 1500, 
    config.get("max_tokens_padrao", 500), 
    50
)

modelo = st.sidebar.selectbox(
    "ğŸ§  Modelo:",
    ["gpt-4o", "gpt-4", "gpt-3.5-turbo"],
    index=0
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AÃ‡Ã•ES DA SIDEBAR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.sidebar.write("### ğŸ› ï¸ AÃ§Ãµes")

col1, col2 = st.sidebar.columns(2)
with col1:
    if st.button("ğŸ—‘ï¸ Limpar", use_container_width=True):
        st.session_state["lista_mensagens"] = []
        st.rerun()

with col2:
    if st.button("ğŸ”„ Recarregar", use_container_width=True):
        st.cache_data.clear()
        st.rerun()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INFORMAÃ‡Ã•ES DOS ARQUIVOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.sidebar.write("---")
st.sidebar.write("### ğŸ“ Arquivos RAG")
st.sidebar.caption("ğŸ“š Base: conhecimento/base_conhecimento.txt")
st.sidebar.caption("ğŸ“„ Prompt: prompts/prompt_sistema.txt")
st.sidebar.caption("âš™ï¸ Config: config/bot_config.json")

# OpÃ§Ã£o para visualizar base de conhecimento
if st.sidebar.checkbox("ğŸ‘€ Ver Base de Conhecimento"):
    with st.sidebar.expander("ğŸ“– ConteÃºdo Carregado"):
        st.text_area(
            "Base atual:", 
            base_conhecimento['texto_completo'][:500] + "..." if len(base_conhecimento['texto_completo']) > 500 else base_conhecimento['texto_completo'],
            height=200,
            disabled=True
        )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FUNÃ‡ÃƒO PARA GERAR RESPOSTA COM RAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def gerar_resposta_com_rag(pergunta_usuario):
    """Gera resposta usando RAG - busca contexto relevante antes de responder"""
    
    # 1. RETRIEVAL - Busca informaÃ§Ã£o relevante
    contexto_relevante = buscar_contexto_relevante(
        pergunta_usuario, 
        base_conhecimento, 
        max_contexto
    )
    
    # 2. AUGMENTATION - ConstrÃ³i prompt com contexto
    mensagem_com_contexto = f"""CONTEXTO DA BASE DE CONHECIMENTO:
{contexto_relevante}

PERGUNTA DO USUÃRIO: {pergunta_usuario}

InstruÃ§Ãµes: Responda a pergunta usando EXCLUSIVAMENTE as informaÃ§Ãµes do CONTEXTO acima. Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga que nÃ£o possui essa informaÃ§Ã£o na base de conhecimento atual."""
    
    # 3. GENERATION - Gera resposta com o contexto
    mensagens_completas = [
        {"role": "system", "content": prompt_sistema},
        {"role": "user", "content": mensagem_com_contexto}
    ]
    
    return mensagens_completas, contexto_relevante

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INTERFACE PRINCIPAL DO CHAT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Inicializa mensagens
if "lista_mensagens" not in st.session_state:
    st.session_state["lista_mensagens"] = []

# Exibe histÃ³rico
for msg in st.session_state["lista_mensagens"]:
    if msg["role"] == "user":
        st.chat_message("user", avatar="ğŸ‘¤").write(msg["content"])
    elif msg["role"] == "assistant":
        st.chat_message("assistant", avatar="ğŸ¤–").write(msg["content"])
    elif msg["role"] == "context":
        # Exibe contexto usado (opcional)
        with st.expander("ğŸ” Contexto RAG utilizado"):
            st.text(msg["content"])

# Entrada do usuÃ¡rio
mensagem_usuario = st.chat_input("ğŸ’­ FaÃ§a sua pergunta sobre Streamlit...")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROCESSAMENTO COM RAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if mensagem_usuario:
    # Exibe mensagem do usuÃ¡rio
    st.chat_message("user", avatar="ğŸ‘¤").write(mensagem_usuario)
    st.session_state["lista_mensagens"].append({
        "role": "user", 
        "content": mensagem_usuario
    })
    
    # Processa com RAG
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        with st.spinner("ğŸ” Buscando na base de conhecimento..."):
            try:
                # Gera resposta com RAG
                mensagens_rag, contexto_usado = gerar_resposta_com_rag(mensagem_usuario)
                
                # Chamada Ã  API
                resposta = client.chat.completions.create(
                    model=modelo,
                    messages=mensagens_rag,
                    temperature=temperatura,
                    max_tokens=max_tokens,
                    top_p=0.9
                )
                
                resposta_ia = resposta.choices[0].message.content
                
                # Exibe resposta
                st.write(resposta_ia)
                
                # Salva no histÃ³rico
                st.session_state["lista_mensagens"].append({
                    "role": "assistant", 
                    "content": resposta_ia
                })
                
                # Opcionalmente salva o contexto usado
                st.session_state["lista_mensagens"].append({
                    "role": "context",
                    "content": contexto_usado
                })
                
            except Exception as e:
                st.error(f"âŒ Erro na API: {str(e)}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INFORMAÃ‡Ã•ES INICIAIS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if len(st.session_state["lista_mensagens"]) == 0:
    st.info("ğŸ¤– OlÃ¡! Sou seu assistente RAG especializado em Streamlit. FaÃ§a perguntas sobre desenvolvimento, componentes, configuraÃ§Ãµes e muito mais!")
    
    # Exemplos de perguntas
    st.write("**ğŸ’¡ Exemplos de perguntas:**")
    exemplos = [
        "Como usar session_state no Streamlit?",
        "Como criar uma sidebar?", 
        "Qual a diferenÃ§a entre st.cache_data e st.cache_resource?",
        "Como fazer deploy de uma aplicaÃ§Ã£o Streamlit?",
        "Como usar st.chat_message()?",
        "Quais sÃ£o os widgets de entrada disponÃ­veis?"
    ]
    
    for exemplo in exemplos:
        if st.button(f"ğŸ“ {exemplo}", key=exemplo):
            st.session_state["pergunta_exemplo"] = exemplo
            st.rerun()
    
    # Se clicou em um exemplo, processa
    if "pergunta_exemplo" in st.session_state:
        mensagem_usuario = st.session_state["pergunta_exemplo"]
        del st.session_state["pergunta_exemplo"]  # Remove para nÃ£o repetir

# RodapÃ©
st.write("---")
st.caption("ğŸ“š Respostas baseadas na base de conhecimento â€¢ ğŸ” Sistema RAG ativo")